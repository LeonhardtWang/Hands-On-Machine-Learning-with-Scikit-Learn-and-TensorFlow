{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、维数诅咒  \n",
    "维数越高，样本之间的距离越远，这会导致数据集有非常稀疏的风险"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、降维的主要方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 投影**  \n",
    "并不总是最好的方法，例如：将瑞士卷数据集投影到二维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 流形学习**  \n",
    "通过对训练集所处的流形进行建模的许多降维算法，称作流形学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、PCA  \n",
    "首先识别出位于数据最近的超平面，然后将数据投影到上面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1 保留方差**  \n",
    "方法一：选择能够保留最大方差的轴  \n",
    "方法二：选择能够使原始数据集和它在轴上投影之间均方距离最小的轴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 主成分**  \n",
    "$X=U\\cdot∑\\cdot V^{T}$  \n",
    "$X_{m\\times n}$，$U_{m\\times m}$，$∑_{m\\times n}$，$V^{T}_{n\\times n}$  \n",
    "$X$：训练集矩阵  \n",
    "$V^{T}$：第i列即为第i个主成分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建三维数据集\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(4)\n",
    "m = 60\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "X = np.empty((m, 3))\n",
    "X[:, 0] = np.cos(angles) + np.sin(angles) / 2 + noise * np.random.rand(m) / 2\n",
    "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用SVD获得训练集的所有主成分\n",
    "X_centered = X - X.mean(axis=0)\n",
    "U, s, V = np.linalg.svd(X_centered)\n",
    "c1 = V.T[:, 0]\n",
    "c2 = V.T[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60, 60), (3,), (3, 3))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape, s.shape, V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.zeros((60, 3), dtype=complex) # dtype=complex：复数类型\n",
    "S[:3, :3] = np.diag(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3 向下投影至d维**  \n",
    "$X_{d-proj}=X\\cdot W_{d}$  \n",
    "$W_{d}$：取出$V^{T}$中前$d$列向量组成的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = V.T[:, :2]\n",
    "X2D = X_centered.dot(w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3 用Scikit-Learn完成**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X2D = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.4 方差解释率**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84788438 0.14210985]\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.5 选择恰当的维数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上面找到保留95%方差的维数，然后再设置n_components=d执行PCA降维\n",
    "# 另一种更好的方法：设置n_components为0.0到1.0之间的数，表示保留的方差解释率\n",
    "pca = PCA(n_components=0.95)\n",
    "X_reduced = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.6 PCA用于压缩**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取minst数据集\n",
    "from sklearn.datasets import fetch_mldata\n",
    "\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "X_mnist = mnist[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 154)\n",
    "X_mnist_reduced = pca.fit_transform(X_mnist)\n",
    "X_mnist_recovered = pca.inverse_transform(X_mnist_reduced) # 解压缩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA逆变换，解压缩回原来的维数：  \n",
    "$X_{recovered}=X_{d-proj}\\cdot W_{d}^{T}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.7 增量PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 之前执行PCA时，会一次性将整个数据集写入内存，增量PCA解决这个问题\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "\n",
    "n_batches = 100\n",
    "inc_pca = IncrementalPCA(n_components=154)\n",
    "for X_batch in np.array_split(X_mnist, n_batches):\n",
    "    inc_pca.partial_fit(X_batch)\n",
    "\n",
    "X_mnist_reduced = inc_pca.transform(X_mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LeoWang\\Anaconda3\\lib\\site-packages\\sklearn\\decomposition\\incremental_pca.py:259: RuntimeWarning: invalid value encountered in true_divide\n",
      "  explained_variance_ratio = S ** 2 / np.sum(col_var * n_total_samples)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IncrementalPCA(batch_size=700, copy=True, n_components=154, whiten=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 或者，使用Numpy的memmap类，允许你操作存储在磁盘上的二进制文件中的大型数组\n",
    "# 由于增量PCA类在任何给定时间都用数组的一小部分，所以内存在节制下，可以调用fit()方法\n",
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "filename = os.path.join(path, \"my_mnist.data\").replace('\\\\', '/')\n",
    "m, n = X_mnist.shape\n",
    "X_mm = np.memmap(filename, dtype=\"float32\", mode=\"write\", shape=(m,n))\n",
    "\n",
    "batch_size = m // n_batches\n",
    "inc_pca = IncrementalPCA(n_components=154, batch_size=batch_size)\n",
    "inc_pca.fit(X_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.8 随机PCA**  \n",
    "计算复杂度为：$O(m\\times d^{2})+O(d^{3})$  \n",
    "而不是先前的：$O(m\\times n^{2})+O(n^{3})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_pca = PCA(n_components=154, svd_solver=\"randomized\")\n",
    "X_reduced = rnd_pca.fit_transform(X_mnist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、核PCA  \n",
    "可以执行复杂的非线性投影以降低维度  \n",
    "通常可以很好地保存投影后地样本簇，或者擅长展开在扭曲流形附件地数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.04)\n",
    "X_reduced = rbf_pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1 选择核函数和调整超参数**  \n",
    "虽然PCA是无监督学习算法，但通常它是监督学习任务地准备步骤，  \n",
    "因此可以通过网格搜索寻找在任务上表现最好的核函数和超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "\n",
    "X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
    "y = t > 6.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('kpca', KernelPCA(alpha=1.0, coef0=1, copy_X=True, degree=3, eigen_solver='auto',\n",
       "     fit_inverse_transform=False, gamma=None, kernel='linear',\n",
       "     kernel_params=None, max_iter=None, n_components=2, n_jobs=1,\n",
       "     random_state=None, remove_zero_eig=False, tol=0)), ('log_reg', LogisticRegre...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'kpca__gamma': array([0.03   , 0.03222, 0.03444, 0.03667, 0.03889, 0.04111, 0.04333,\n",
       "       0.04556, 0.04778, 0.05   ]), 'kpca__kernel': ['rbf', 'sigmoid']}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline([\n",
    "        (\"kpca\", KernelPCA(n_components=2)),\n",
    "        (\"log_reg\", LogisticRegression())\n",
    "    ])\n",
    "\n",
    "param_grid = [{\n",
    "        \"kpca__gamma\":np.linspace(0.03, 0.05, 10),\n",
    "        \"kpca__kernel\":[\"rbf\", \"sigmoid\"]\n",
    "    }]\n",
    "\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=3)\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'kpca__gamma': 0.043333333333333335, 'kpca__kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一种方法：选择核函数和超参数产生最小重构误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.0433, \n",
    "                   fit_inverse_transform=True)\n",
    "X_reduced = rbf_pca.fit_transform(X)\n",
    "X_preimage = rbf_pca.inverse_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.78630879576613"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算重构误差\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(X, X_preimage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、LLE（局部线性嵌入）  \n",
    "另一种强大的非线性降维技术，是一种不依赖于投影的流形学习技术"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
    "X_reduced = lle.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLE步骤1：对局部关系进行线性建模  \n",
    "$\\pmb{\\widehat{W}}=\\underset{\\pmb{W}}{argmin}\\sum_{i=1}^{m}\\left \\| \\pmb{x}^{(i)}-\\sum_{j=1}^{m}w_{i,j}\\pmb{x}^{(j)} \\right \\|^{2}$  \n",
    "$s.t.\\left\\{\\begin{matrix}\n",
    "w_{i,j}=0 &if\\,\\pmb{x}^{(j)}不是\\pmb{x}^{(i)}的k近邻中的一个 \\\\ \n",
    "\\sum_{j=1}^{m}w_{i,j}=1 & i=1,2,\\cdots ,m\n",
    "\\end{matrix}\\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLE步骤2：保留关系的同时降低维度  \n",
    "$\\widehat{\\pmb{Z}}=\\underset{\\pmb{Z}}{argmin}\\sum_{i=1}^{m}\\left \\| \\pmb{z}^{(i)}-\\sum_{j=1}^{m}\\widehat{w}_{i,j}\\pmb{z}^{(j)} \\right \\|^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 六、其他降维技巧  \n",
    "**多维缩放（MDS）**：试图保持样本间距离同时降低维数  \n",
    "**Isomap**：通过将每个样本连接到最近邻来构造近邻图，然后试图保持样本之间的测地距离同时降低维度  \n",
    "**t-分布随机近邻嵌入（t-SNE）**：试图让相似样本接近，不相似样本远离的同时降低维数。主要用于可视化，尤其用于可视化高维空间中的样本簇  \n",
    "**线性判别分析（LDA）**：实际为一种分类算法，在训练过程中，它会学习类之间最有辨别性的轴，然后使用这些轴定义为投影数据的超平面。好处是投影会尽可能保持类之间的距离，所以在执行另一种分类算法（如SVM分类器）之前，LDA是降低维数的好方法"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
